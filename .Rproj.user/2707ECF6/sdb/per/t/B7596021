{
    "collab_server" : "",
    "contents" : "library(twitteR)\nlibrary(ROAuth)\nlibrary(plyr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(ggplot2)\n\nsetup_twitter_oauth(  'SfOXeq5JkZfGhK4MkEpzIExqB',\n                      '6RdpLsQlzxUXeNS84PlrfKedJ0iuXN1cCbpU0AZx65kgcHsdta',\n                    '1009310760-BMJia7jbEmhA0mMWOfcdt6U6w16U8xb4OEtmZOd',\n                    'xbkxGbH2e4z4pLFuoTvAMOXzwdR1ccGycFP9sCTJxX9aW')\nsetup_twitter_oauth(\"API key\", \"API secret\", \"Access token\", \"Access secret\")\n\nreqURL <- 'https://api.twitter.com/oauth/request_token'\naccessURL <- 'https://api.twitter.com/oauth/access_token'\nauthURL <- 'https://api.twitter.com/oauth/authorize'\nconsumerKey <- 'SfOXeq5JkZfGhK4MkEpzIExqB' \nconsumerSecret <- '6RdpLsQlzxUXeNS84PlrfKedJ0iuXN1cCbpU0AZx65kgcHsdta' \nCred <- OAuthFactory$new(consumerKey=consumerKey,\n                         consumerSecret=consumerSecret,\n                         requestURL=reqURL,\n                         accessURL=accessURL,\n                         authURL=authURL)\nCred$handshake(cainfo = system.file('CurlSSL', 'cacert.pem', package = 'RCurl'))\n\nsave(Cred, file='twitter authentication.Rdata')\nload('twitter authentication.Rdata') #Once you launch the code first time, you can start from this line in the future (libraries should be connected)\nregisterTwitterOAuth(Cred)\n\n#the function of tweets accessing and analyzing\nsearch <- function(searchterm)\n{\n  #access tweets and create cumulative file\n  \n  list <- searchTwitter(searchterm, cainfo='cacert.pem', n=1500)\n  df <- twListToDF(list)\n  df <- df[, order(names(df))]\n  df$created <- strftime(df$created, '%Y-%m-%d')\n  if (file.exists(paste(searchterm, '_stack.csv'))==FALSE) write.csv(df, file=paste(searchterm, '_stack.csv'), row.names=F)\n  \n  #merge last access with cumulative file and remove duplicates\n  stack <- read.csv(file=paste(searchterm, '_stack.csv'))\n  stack <- rbind(stack, df)\n  stack <- subset(stack, !duplicated(stack$text))\n  write.csv(stack, file=paste(searchterm, '_stack.csv'), row.names=F)\n  \n  #evaluation tweets function\n  score.sentiment <- function(sentences, pos.words, neg.words, .progress='none')\n  {\n    require(plyr)\n    require(stringr)\n    scores <- laply(sentences, function(sentence, pos.words, neg.words){\n      sentence <- gsub('[[:punct:]]', \"\", sentence)\n      sentence <- gsub('[[:cntrl:]]', \"\", sentence)\n      sentence <- gsub('\\d+', \"\", sentence)\n      sentence <- tolower(sentence)\n      word.list <- str_split(sentence, '\\s+')\n      words <- unlist(word.list)\n      pos.matches <- match(words, pos.words)\n      neg.matches <- match(words, neg.words)\n      pos.matches <- !is.na(pos.matches)\n      neg.matches <- !is.na(neg.matches)\n      score <- sum(pos.matches) - sum(neg.matches)\n      return(score)\n    }, pos.words, neg.words, .progress=.progress)\n    scores.df <- data.frame(score=scores, text=sentences)\n    return(scores.df)\n  }\n  \n  pos <- scan('C:/___________/positive-words.txt', what='character', comment.char=';') #folder with positive dictionary\n  neg <- scan('C:/___________/negative-words.txt', what='character', comment.char=';') #folder with negative dictionary\n  pos.words <- c(pos, 'upgrade')\n  neg.words <- c(neg, 'wtf', 'wait', 'waiting', 'epicfail')\n  \n  Dataset <- stack\n  Dataset$text <- as.factor(Dataset$text)\n  scores <- score.sentiment(Dataset$text, pos.words, neg.words, .progress='text')\n  write.csv(scores, file=paste(searchterm, '_scores.csv'), row.names=TRUE) #save evaluation results into the file\n  \n  #total evaluation: positive / negative / neutral\n  stat <- scores\n  stat$created <- stack$created\n  stat$created <- as.Date(stat$created)\n  stat <- mutate(stat, tweet=ifelse(stat$score > 0, 'positive', ifelse(stat$score < 0, 'negative', 'neutral')))\n  by.tweet <- group_by(stat, tweet, created)\n  by.tweet <- summarise(by.tweet, number=n())\n  write.csv(by.tweet, file=paste(searchterm, '_opin.csv'), row.names=TRUE)\n  \n  #create chart\n  ggplot(by.tweet, aes(created, number)) + geom_line(aes(group=tweet, color=tweet), size=2) +\n    geom_point(aes(group=tweet, color=tweet), size=4) +\n    theme(text = element_text(size=18), axis.text.x = element_text(angle=90, vjust=1)) +\n    #stat_summary(fun.y = 'sum', fun.ymin='sum', fun.ymax='sum', colour = 'yellow', size=2, geom = 'line') +\n    ggtitle(searchterm)\n  \n  ggsave(file=paste(searchterm, '_plot.jpeg'))\n  \n}\n\nsearch(\"______\") #enter keyword",
    "created" : 1503316220392.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "129419971",
    "id" : "B7596021",
    "lastKnownWriteTime" : 1503416373,
    "last_content_update" : 1503416374547,
    "path" : "D:/DataAnalytics/Sentiment_analysis/code2.R",
    "project_path" : "code2.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}